{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsalvado/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import backtrader as bt\n",
    "\n",
    "from btgym import BTgymEnv, BTgymDataset\n",
    "from btgym.strategy.observers import Reward, Position, NormPnL\n",
    "from btgym.algorithms import Launcher, A3C\n",
    "\n",
    "from btgym.research import DevStrat_4_6\n",
    "\n",
    "from btgym.algorithms.policy import Aac1dPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Under `DATASET` settings uncomment either synthetic data file (simple sine wave) - \n",
    "# solves it for less than 200K env. steps, \n",
    "# or: 1 month real EURUSD data file - solves it for about 14M env. steps (for vanilla A3C).\n",
    "#\n",
    "# Ignore `Data_master `reset()...` and `Dataset not ready...` warnings\n",
    "#\n",
    "# To visualise training and results point tensorboard to: User_home/tmp/test_gym_a3c\n",
    "#\n",
    "# Refer to environment renderings to see actual trades, orders etc.\n",
    "#\n",
    "# Refer to `DevStrat_4_6` comments and Docs for state and reward shaping information.\n",
    "#\n",
    "# For UNREAL: enable aux. `Value Replay` task, `Pixel Change Control` task or both, \n",
    "# note more sample-efficient convergence.\n",
    "# It's currently not recommended to turn on 'Reward Prediction` task as it seems to hurt performance for BTgym;\n",
    "# maybe bug.\n",
    "#\n",
    "# Toy trading settings details for single episode:\n",
    "# Trading pair: EUR/USD\n",
    "# Initial cash: 2K USD, leverage 1:10, single stake size: 5K, can add position\n",
    "# Commission is set to imitate spread\n",
    "# Stop trading if lost 5% of initial amount (-100USD)\n",
    "# Stop trading if reached 10% profit (+200USD)\n",
    "# Profit is considered as `broker value` at trade episode end.\n",
    "# Episode start times sampled randomly from dataset date/time range in a way to ensure\n",
    "# contionious trade within a week, i.e. starts on fridays, holidays are excluded.\n",
    "# Trade maximum for 23 hours 55 mins, start trading in random time of day (single overnight is ok)\n",
    "# Actions are market orders only: sell, buy, close or do_nothing\n",
    "# Allowed to issue orders every 10th minute from beginning of episode tradetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set backtesting engine parameters:\n",
    "\n",
    "MyCerebro = bt.Cerebro()\n",
    "\n",
    "MyCerebro.addstrategy(\n",
    "    DevStrat_4_6,\n",
    "    drawdown_call=5, # max % to loose, in percent of initial cash\n",
    "    target_call=10,  # max % to win, same\n",
    "    skip_frame=10,\n",
    ")\n",
    "# Set leveraged account:\n",
    "MyCerebro.broker.setcash(2000)\n",
    "MyCerebro.broker.setcommission(commission=0.0001, leverage=10.0) # commisssion to imitate spread\n",
    "MyCerebro.addsizer(bt.sizers.SizerFix, stake=5000,)  \n",
    "\n",
    "#MyCerebro.addanalyzer(bt.analyzers.DrawDown)\n",
    "\n",
    "# Visualisations for reward, position and PnL dynamics:\n",
    "MyCerebro.addobserver(Reward)\n",
    "MyCerebro.addobserver(Position)\n",
    "MyCerebro.addobserver(NormPnL)\n",
    "\n",
    "MyDataset = BTgymDataset(\n",
    "    #filename='.data/DAT_ASCII_EURUSD_M1_201703.csv',\n",
    "    #filename='./data/DAT_ASCII_EURUSD_M1_201704.csv',\n",
    "    filename='./data/test_sine_1min_period256_delta0002.csv',\n",
    "    start_weekdays={0, 1, 2, 3},\n",
    "    episode_duration={'days': 0, 'hours': 23, 'minutes': 55},\n",
    "    start_00=False,\n",
    "    time_gap={'hours': 6},\n",
    ")\n",
    "\n",
    "env_config = dict(\n",
    "    class_ref=BTgymEnv,\n",
    "    kwargs=dict(\n",
    "        dataset=MyDataset,\n",
    "        engine=MyCerebro,\n",
    "        render_modes=['episode', 'human','external'],\n",
    "        render_state_as_image=True,\n",
    "        render_ylabel='OHL_diff.',\n",
    "        render_size_episode=(12,8),\n",
    "        render_size_human=(9, 4),\n",
    "        render_size_state=(11, 3),\n",
    "        render_dpi=75,\n",
    "        port=5000,\n",
    "        data_port=4999,\n",
    "        connect_timeout=60,\n",
    "        verbose=0,  # better be 0\n",
    "    )\n",
    ")\n",
    "\n",
    "cluster_config = dict(\n",
    "    host='127.0.0.1',\n",
    "    port=12230,\n",
    "    num_workers=6,  # Set according CPU's available \n",
    "    num_ps=1,\n",
    "    num_envs=1,  # do not change yet\n",
    "    log_dir=os.path.expanduser('~/tmp/test_gym_a3c'),\n",
    ")\n",
    "\n",
    "policy_config = dict(\n",
    "    class_ref=Aac1dPolicy,\n",
    "    kwargs={}\n",
    ")\n",
    "\n",
    "trainer_config = dict(\n",
    "    class_ref=A3C,\n",
    "    kwargs=dict(\n",
    "        opt_learn_rate=[1e-4, 1e-4], # or random log-uniform range, values > 2e-4 can ruin training \n",
    "        opt_end_learn_rate=1e-5,\n",
    "        opt_decay_steps=100*10**6,\n",
    "        model_gae_lambda=0.95,\n",
    "        model_beta=[0.05, 0.01], # Entropy reg, random log-uniform\n",
    "        rollout_length=20,\n",
    "        time_flat=False,\n",
    "        model_summary_freq=100,\n",
    "        episode_summary_freq=5,\n",
    "        env_render_freq=20,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "launcher = Launcher(\n",
    "    cluster_config=cluster_config,\n",
    "    env_config=env_config,\n",
    "    trainer_config=trainer_config,\n",
    "    policy_config=policy_config,\n",
    "    test_mode=False,\n",
    "    max_env_steps=100*10**6,\n",
    "    root_random_seed=0,\n",
    "    purge_previous=1,  # ask to override previously saved model and logs\n",
    "    verbose=0  # 0 or 1\n",
    ")\n",
    "\n",
    "# Train it:\n",
    "launcher.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}